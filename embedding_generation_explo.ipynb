{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embedding generation explo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIhjgTr04eeu"
      },
      "source": [
        "Hindi Bhojpuri k liye word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FfrBBhp4W66",
        "outputId": "ba4b6d90-3788-4d18-d2bd-fe6ba374a996"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI1lX-gA5_gu"
      },
      "source": [
        "# below used file is the file same as used for training of data ( word - NER tag ) type\n",
        "# this is used to generate word2vec embeddings and needs to be done for both hindi and bhojpuri\n",
        "f = open ('/content/drive/My Drive/explo/bhoj_chunk.txt','r')\n",
        "lines = f.readlines()\n",
        "print(lines[0].split('\\t'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MWpUYEk7i4Y"
      },
      "source": [
        "words = []\n",
        "sents = []\n",
        "sent = []\n",
        "for line in lines:\n",
        "  line = line.split('\\t')\n",
        "  if line[0]!='\\n':\n",
        "    word = line[0].split('\\ufeff')\n",
        "    if len(word)>1:\n",
        "      word = word[1]\n",
        "    else:\n",
        "      word = word[0]\n",
        "    if word == '।':\n",
        "      sent.append(word)\n",
        "      sents.append(sent)\n",
        "      sent=[]\n",
        "    else:\n",
        "      sent.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiInygcF749f"
      },
      "source": [
        "print(len(sents))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0Gpd2ik-mMS"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "word2vec_skipgram = Word2Vec(sents, size=100,window=5,min_count=2, negative=10,sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72ZOsvC7_EHB"
      },
      "source": [
        "# enter file location where you wannt to save your embeddings. Above code has to be used for once, bcoz once you generate and save these embeddings, they can be used directly\n",
        "word2vec_skipgram.wv.save_word2vec_format('/content/drive/My Drive/explo/bhoj.txt', binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5M9iyyuB9No"
      },
      "source": [
        "f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gKe08cqCDfN"
      },
      "source": [
        "f = open ('/content/drive/My Drive/explo/hindi_treebank_iob.txt','r')\n",
        "lines = f.readlines()\n",
        "print(lines[0].split('\\t'))\n",
        "\n",
        "words = []\n",
        "sents = []\n",
        "sent = []\n",
        "for line in lines:\n",
        "  line = line.split('\\t')\n",
        "  if line[0]!='\\n':\n",
        "    word = line[0].split('\\ufeff')\n",
        "    if len(word)>1:\n",
        "      word = word[1]\n",
        "    else:\n",
        "      word = word[0]\n",
        "    if word == '।':\n",
        "      sent.append(word)\n",
        "      sents.append(sent)\n",
        "      sent=[]\n",
        "    else:\n",
        "      sent.append(word)\n",
        "\n",
        "print(len(sents))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAewUybsCit0"
      },
      "source": [
        "print(sents[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3b-W2n1CaJ6"
      },
      "source": [
        "word2vec_skipgram = Word2Vec(sents, size=100,window=5,min_count=2, negative=10,sg=1)\n",
        "word2vec_skipgram.wv.save_word2vec_format('/content/drive/My Drive/explo/hindi.txt', binary=False)\n",
        "word2vec_skipgram.wv.save_word2vec_format('/content/drive/My Drive/explo/hindi.emb', binary=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqyuG5wyDRDw"
      },
      "source": [
        "# clone this in some folder and this has to be done once\n",
        "%cd /content/drive/MyDrive/explo\n",
        "!git clone https://github.com/artetxem/vecmap.git\n",
        "%cd vecmap\n",
        "\n",
        "# The word2vec embeddings generated have to be moved to this folder ( .emb files)\n",
        "# hindi.emb and bhoj.emb are word2vec embeddings generated using above code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-siUDgIEegY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbcd0a7-cba2-46d1-ab7c-02768370aaf3"
      },
      "source": [
        "!python3 map_embeddings.py --unsupervised hindi.emb bhoj.emb SRC_MAPPED_H_to_B.EMB TRG_MAPPED_H_to_B.EMB --cuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'map_embeddings.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FxGSLJ4A8Af",
        "outputId": "31082302-c92c-4717-d615-f696c6a8e5c2"
      },
      "source": [
        "!pip install --user keras\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "!git clone https://github.com/ziadloo/attention_keras.git\n",
        "!pip install keras-attention\n",
        "!pip install keras-self-attention\n",
        "!pip install keras==2.2.4        \n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "!pip install tensorflow==1.14.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.15.0)\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-l3xgwhs8\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-l3xgwhs8\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=453644b2aa699048218ae92a7021f7171a0f77c8100fd24afd47081de0ffa8d7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-slhfeku1/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n",
            "Cloning into 'attention_keras'...\n",
            "remote: Enumerating objects: 199, done.\u001b[K\n",
            "remote: Total 199 (delta 0), reused 0 (delta 0), pack-reused 199\u001b[K\n",
            "Receiving objects: 100% (199/199), 47.20 MiB | 32.86 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n",
            "Collecting keras-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/9a/e9/1373dff1b2dd7cb45208d224c84d7a2bddcf0519f01d98f07d8ce15dc4be/keras_attention-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-attention) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-attention) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-attention) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-attention) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-attention) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-attention) (1.15.0)\n",
            "Installing collected packages: keras-attention\n",
            "Successfully installed keras-attention-1.0.0\n",
            "Collecting keras-self-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/34/e21dc6adcdab2be03781bde78c6c5d2b2136d35a1dd3e692d7e160ba062a/keras-self-attention-0.49.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-self-attention) (1.15.0)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.49.0-cp37-none-any.whl size=19468 sha256=3ba08bfbc8ab3613a4f1bee1e3df4a791643087f2609b59e2bebe44f6ce2a231\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/9d/c5/26693a5092d9313daeae94db04818fc0a2b7a48ea381989f34\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.49.0\n",
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Installing collected packages: keras-applications, keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.2.4 keras-applications-1.0.8\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-nlbyxjhv\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-nlbyxjhv\n",
            "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=e26a04e75fa0f07ef4f551519e7443592bdc06206a2fc1c4636c1fdc9c438061\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jbz4txgv/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/28/96efba1a516cdacc2e2d6d081f699c001d414cc8ca3250e6d59ae657eb2b/tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3MB 51kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.19.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.36.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.32.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 49.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (56.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.1)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaxUtct6BXji",
        "outputId": "49432251-b470-4302-f5b4-35ed47a2968e"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import numpy\n",
        "from collections import Counter\n",
        "from keras.models import *\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dropout, Dense,concatenate\n",
        "from keras_contrib.layers import CRF\n",
        "from keras_contrib.losses import crf_loss\n",
        "from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "from keras_contrib.datasets import conll2000\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from collections import Counter\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import model_from_json\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8QKRotnBcPz"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "EMBED_DIM = 400\n",
        "BiRNN_UNITS = 400\n",
        "max_len_char=-1\n",
        "max_len=None\n",
        "chunking_file_path = '/content/drive/My Drive/explo/bhoj_chunk.txt'\n",
        "output_file = 'output.txt'\n",
        "def load_data(chunking_file_path, min_freq=1):\n",
        "\n",
        "    file_train = _parse_data(open(chunking_file_path))\n",
        "    print (len(file_train))\n",
        "    count1=0\n",
        "    for i in file_train:\n",
        "        count1=count1+len(i)\n",
        "    print (count1)\n",
        "    word_counts = Counter(row[0].lower() for sample in file_train for row in sample if len(row)>=3)\n",
        "    vocab = ['<pad>', '<unk>']\n",
        "    vocab += [w for w, f in iter(word_counts.items()) if f >= min_freq]\n",
        "    pos_tags = sorted(list(set(row[1] for sample in file_train for row in sample if len(row)>=3)))\n",
        "    chunk_tags = sorted(list(set(row[2] for sample in file_train for row in sample if len(row)>=3)))\n",
        "    characters = sorted(list(set(i for sample in file_train for row in sample if len(row)>=3 for i in row[0])))\n",
        "    characters.insert(0,'<unk>')\n",
        "    characters.insert(0,'<pad>')\n",
        "    pos_tags.insert(0,'<unk>')\n",
        "    pos_tags.insert(0,'<pad>')\n",
        "    global max_len_char\n",
        "    for sample in file_train:\n",
        "        for row in sample:\n",
        "            if len(row) >= 3:\n",
        "                max_len_char=max(max_len_char,len(row[0]))\n",
        "    train = _process_data(file_train, vocab, pos_tags, chunk_tags,characters)\n",
        "    return train, (vocab, pos_tags, chunk_tags,characters)\n",
        "def _parse_data(fh):\n",
        "    string = fh.read()\n",
        "    # print(string)\n",
        "    data = []\n",
        "    for sample in string.strip().split('\\n\\n'):\n",
        "        data.append([row.split() for row in sample.split('\\n')])\n",
        "    fh.close()\n",
        "    return data\n",
        "def pad_words(l,max_len_char):\n",
        "    length=len(l)\n",
        "    l1=[0 for i in range(max_len_char-length)]\n",
        "    l=l1+l\n",
        "    return l\n",
        "def _process_data(data, vocab, pos_tags, chunk_tags, characters, onehot=False):\n",
        "    global max_len\n",
        "    if max_len is None:\n",
        "        max_len = max(len(s) for s in data)\n",
        "    word2idx = dict((w, i) for i, w in enumerate(vocab))\n",
        "    x = [[word2idx.get(w[0].lower(), 1) for w in s if len(w)>=3] for s in data]\n",
        "    y_pos = [[pos_tags.index(w[1]) for w in s if len(w)>=3] for s in data]\n",
        "    y_chunk = [[chunk_tags.index(w[2]) for w in s if len(w)>=3] for s in data]\n",
        "    defaultvalue=[0 for i in range(max_len_char)]\n",
        "    x = pad_sequences(x, max_len)\n",
        "    y_pos = pad_sequences(y_pos, max_len, value=0)\n",
        "    y_chunk = pad_sequences(y_chunk, max_len, value=0)\n",
        "\n",
        "    if onehot:\n",
        "        y_pos = numpy.eye(len(pos_tags), dtype='float32')[y]\n",
        "        y_chunk = numpy.eye(len(chunk_tags), dtype='float32')[y]\n",
        "    else:\n",
        "        y_pos = numpy.expand_dims(y_pos, 2)\n",
        "        y_chunk = numpy.expand_dims(y_chunk, 2)\n",
        "    return x, y_pos, y_chunk\n",
        "def tocharacter(characters,vocab,X_train):\n",
        "    ''' Function to create word embedding into character embedding'''\n",
        "    char2idx= dict((w,i) for i,w in enumerate(characters))\n",
        "    idx2word= dict((i, w) for i, w in enumerate(vocab)) \n",
        "    l=[]\n",
        "    for s in X_train:\n",
        "        l1=[]\n",
        "        for w in s:\n",
        "            if (idx2word[w]=='<pad>'):\n",
        "                l1.append([0]*max_len_char)\n",
        "                continue\n",
        "            if (idx2word[w]=='<unk>'):\n",
        "                l1.append([1]*max_len_char)\n",
        "                continue\n",
        "            l2=[]\n",
        "            for c in idx2word[w]:\n",
        "                l2.append(char2idx.get(c,1))\n",
        "            l2=pad_words(l2,max_len_char)\n",
        "            l1.append(l2)\n",
        "        l.append(l1)\n",
        "    return numpy.asarray(l)\n",
        "def classification_report(y_true, y_pred, labels):\n",
        "    '''Similar to the one in sklearn.metrics,\n",
        "    reports per classs recall, precision and F1 score'''\n",
        "    y_true = numpy.asarray(y_true).ravel()\n",
        "    y_pred = numpy.asarray(y_pred).ravel()\n",
        "    corrects = Counter(yt for yt, yp in zip(y_true, y_pred) if yt == yp)\n",
        "    y_true_counts = Counter(y_true)\n",
        "    y_pred_counts = Counter(y_pred)\n",
        "    report = ((lab,  # label\n",
        "               corrects[i] / max(1, y_true_counts[i]),  # recall\n",
        "               corrects[i] / max(1, y_pred_counts[i]),  # precision\n",
        "               y_true_counts[i]  # support\n",
        "               ) for i, lab in enumerate(labels))\n",
        "    report = [(l, r, p, 2 * r * p / max(1e-9, r + p), s) for l, r, p, s in report]\n",
        "\n",
        "    print('{:<15}{:>10}{:>10}{:>10}{:>10}\\n'.format('',\n",
        "                                                    'recall',\n",
        "                                                    'precision',\n",
        "                                                    'f1-score',\n",
        "                                                    'support'))\n",
        "    formatter = '{:<15}{:>10.2f}{:>10.2f}{:>10.2f}{:>10d}'.format\n",
        "    for r in report:\n",
        "        print(formatter(*r))\n",
        "    print('')\n",
        "    report2 = list(zip(*[(r * s, p * s, f1 * s) for l, r, p, f1, s in report]))\n",
        "    N = len(y_true)\n",
        "    print(formatter('avg / total',\n",
        "                    sum(report2[0]) / N,\n",
        "                    sum(report2[1]) / N,\n",
        "                    sum(report2[2]) / N, N) + '\\n')\n",
        "    actual = Counter(y_true)\n",
        "    del actual[-1]\n",
        "    accuracy = sum(corrects.values()) / sum(actual.values())\n",
        "    print('Accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR1N4kN1Bfti",
        "outputId": "c75fb359-e865-4db4-f917-728bef54bc6c"
      },
      "source": [
        "train, voc = load_data(chunking_file_path)\n",
        "(X,x_pos,y) = train\n",
        "(vocab, pos_tags, class_labels,characters) = voc\n",
        "f2=open(\"vocabs.txt\",\"w\")\n",
        "f2.write(\" \".join(vocab)+'\\n')\n",
        "f2.write(\" \".join(characters)+'\\n')\n",
        "f2.write(\" \".join(pos_tags)+'\\n')\n",
        "f2.write(\" \".join(class_labels)+'\\n')\n",
        "f2.flush()\n",
        "f2.close()\n",
        "X_char=tocharacter(characters,vocab,X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=2018)\n",
        "X_char_train, X_char_test, __ , __ = train_test_split(X_char, y, test_size=0.3,random_state=2018)\n",
        "x_pos_train,x_pos_test,__, ___ = train_test_split(x_pos, y, test_size=0.3,random_state=2018)\n",
        "print('==== training BiLSTM-CRF ====')\n",
        "print(max_len_char)\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6327\n",
            "87729\n",
            "==== training BiLSTM-CRF ====\n",
            "23\n",
            "369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdTptyt-BiP5"
      },
      "source": [
        "# below mentioned file is the type of embeddings which you want to use:\n",
        "# it will be either the TRG_MAPPED file generated during the mapping or the word2vec embeddings of low resource language\n",
        "# this has to be run everytime except for the baseline\n",
        "f = open ('/content/drive/My Drive/explo/vecmap/TRG_MAPPED_H_to_B.EMB','r')\n",
        "embeddings_index={}\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "embedding_matrix = np.zeros((len(vocab), 100))\n",
        "i = 0\n",
        "for word in vocab:\n",
        "    # values = line.split()\n",
        "    # word = values[0]\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXGrCZQPDoDi",
        "outputId": "e335575a-5d17-4b76-e563-84973eaccfdf"
      },
      "source": [
        "from keras import backend as K\n",
        "# word embedding\n",
        "\n",
        "word_in=Input(shape=(X_train.shape[1],),name='word_in')\n",
        "emb_word=(Embedding(len(vocab), EMBED_DIM//4, weights = [embedding_matrix], mask_zero=True))(word_in)\n",
        "\n",
        "print(X_train.shape[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcBWXYZ0EZk0"
      },
      "source": [
        "# charcter embedding\n",
        "char_in = Input(shape=(X_char_train.shape[1],X_char_train.shape[2]), name='char_in')\n",
        "emb_char=(TimeDistributed(Embedding(input_dim=len(characters),output_dim=10,input_length=max_len_char,mask_zero=True)))(char_in)\n",
        "char_enc=(TimeDistributed(LSTM(units=EMBED_DIM//4,return_sequences=False)))(emb_char)\n",
        "\n",
        "# pos_tag embedding \n",
        "pos_in=Input(shape=(x_pos_train.shape[1],),name='pos_in')\n",
        "pos_word=(Embedding(len(pos_tags), EMBED_DIM//2, mask_zero=True))(pos_in)\n",
        "\n",
        "# concatenating them word+char+pos\n",
        "x = concatenate([emb_word, char_enc, pos_word])\n",
        "\n",
        "#passing to Bi-LSTM layer\n",
        "o=(Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True, dropout=0.2)))(x)\n",
        "#print(o)\n",
        "\n",
        "#Self attention layer\n",
        "o = (SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                       kernel_regularizer=keras.regularizers.l2(1e-4),\n",
        "                       bias_regularizer=keras.regularizers.l1(1e-4),\n",
        "                       attention_regularizer_weight=1e-4,\n",
        "                       name='Attention'))(o)\n",
        "\n",
        "#CRF layer\n",
        "crf = CRF(len(class_labels), sparse_target=True,name='crf')\n",
        "o=(crf)(o)\n",
        "model=Model([word_in,char_in,pos_in],o)\n",
        "model.compile('adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_FYnT4KEfrS",
        "outputId": "6f5eb543-c499-4543-cd8c-91c52229dccf"
      },
      "source": [
        "EPOCHS = 5\n",
        "model.fit([X_train,np.array(X_char_train).reshape(len(X_char_train),max_len,max_len_char),x_pos_train.reshape(len(x_pos_train),max_len)],np.array(y_train).reshape(len(y_train), max_len, 1),batch_size=32, epochs=EPOCHS, validation_split=0.1, verbose=1)\n",
        "model.save('/content/drive/My Drive/explo/bhoj_explo_mapped.bin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3985 samples, validate on 443 samples\n",
            "Epoch 1/5\n",
            "3985/3985 [==============================] - 1279s 321ms/step - loss: 172.6611 - crf_viterbi_accuracy: 0.4053 - val_loss: 144.7133 - val_crf_viterbi_accuracy: 0.6191\n",
            "Epoch 2/5\n",
            "3985/3985 [==============================] - 1269s 319ms/step - loss: 171.6093 - crf_viterbi_accuracy: 0.7887 - val_loss: 144.1524 - val_crf_viterbi_accuracy: 0.8713\n",
            "Epoch 3/5\n",
            "3985/3985 [==============================] - 1273s 320ms/step - loss: 171.3038 - crf_viterbi_accuracy: 0.8915 - val_loss: 143.9943 - val_crf_viterbi_accuracy: 0.9047\n",
            "Epoch 4/5\n",
            "3985/3985 [==============================] - 1285s 322ms/step - loss: 171.1838 - crf_viterbi_accuracy: 0.9144 - val_loss: 143.9079 - val_crf_viterbi_accuracy: 0.9252\n",
            "Epoch 5/5\n",
            "3985/3985 [==============================] - 1290s 324ms/step - loss: 171.0939 - crf_viterbi_accuracy: 0.9301 - val_loss: 143.8508 - val_crf_viterbi_accuracy: 0.9231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx7kutQzSLW5",
        "outputId": "1cd12d54-1e17-4bef-dce5-7b2e4035aaac"
      },
      "source": [
        "#validation\n",
        "model_json = model.to_json()\n",
        "with open(\"model_bhoj_mapped.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "# model1_new.save_weights(\"/content/drive/My Drive/explo/model_bhoj_fine.h5\")\n",
        "# print(\"Saved model to disk\")\n",
        "l4=[0]*max_len_char\n",
        "l4=numpy.asarray(l4)\n",
        "y= model.predict([X_test,np.array(X_char_test).reshape((len(X_char_test),max_len, max_len_char)),x_pos_test.reshape(len(x_pos_test),max_len)])\n",
        "y= y.argmax(-1)\n",
        "y_pred=[]\n",
        "test_y_true=[]\n",
        "ctest_y_pred=[]\n",
        "ctest_y_true=[]\n",
        "\n",
        "#removing padding from the validation\n",
        "for i in range(len(X_test)):\n",
        "    l=[]\n",
        "    l1=[]\n",
        "    for j in range(len(X_test[i])):\n",
        "        if (X_test[i][j]==0):\n",
        "            continue\n",
        "        l.append(y[i][j])\n",
        "        l1.append(y_test[i][j][0])\n",
        "        ctest_y_pred.append(y[i][j])\n",
        "        ctest_y_true.append(y_test[i][j][0])\n",
        "    y_pred.append(l)\n",
        "    test_y_true.append(l1)\n",
        "\n",
        "#converting to numpy array\n",
        "test_y_pred=numpy.asarray(y_pred)\n",
        "test_y_true=numpy.asarray(test_y_true)\n",
        "\n",
        "#writing output for validation\n",
        "f1=open(output_file,'w')\n",
        "for i in range(len(test_y_pred)):\n",
        "    for j in range(len(test_y_pred[i])):\n",
        "        s='|'+'\\t'+'NN'+'\\t'+str(class_labels[test_y_pred[i][j]])+'\\t'+str(class_labels[test_y_true[i][j]])\n",
        "        f1.write(s+'\\n')\n",
        "    f1.write('\\n')\n",
        "f1.flush()\n",
        "f1.close()\n",
        "print('\\n---- Result of BiLSTM-CRF ----\\n')\n",
        "classification_report(ctest_y_true, ctest_y_pred, class_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- Result of BiLSTM-CRF ----\n",
            "\n",
            "                   recall precision  f1-score   support\n",
            "\n",
            "B-BLK                0.78      0.78      0.78       355\n",
            "B-CCP                0.99      0.98      0.98       747\n",
            "B-JJP                0.07      0.58      0.12       160\n",
            "B-NEGP               0.01      1.00      0.03        73\n",
            "B-NNP                0.00      0.00      0.00         0\n",
            "B-NP                 0.95      0.95      0.95      6846\n",
            "B-NST                0.00      0.00      0.00         0\n",
            "B-PRP                0.00      0.00      0.00         0\n",
            "B-RBP                0.89      0.61      0.72        81\n",
            "B-RP                 0.00      0.00      0.00         0\n",
            "B-VGF                0.98      0.87      0.92      3205\n",
            "B-VGINF              0.00      0.00      0.00         9\n",
            "B-VGNF               0.09      0.58      0.16       394\n",
            "B-VGNN               0.00      0.00      0.00         4\n",
            "I-BLK                0.48      0.79      0.59        63\n",
            "I-CCP                0.00      0.00      0.00        11\n",
            "I-JJP                0.22      0.52      0.31       152\n",
            "I-NEGP               0.00      0.00      0.00        27\n",
            "I-NP                 0.98      0.95      0.97      9055\n",
            "I-NST                0.00      0.00      0.00         0\n",
            "I-PRP                0.00      0.00      0.00         0\n",
            "I-RBP                0.00      0.00      0.00        40\n",
            "I-RP                 0.00      0.00      0.00         0\n",
            "I-VGF                0.96      0.89      0.92      2940\n",
            "I-VGINF              0.00      0.00      0.00        10\n",
            "I-VGNF               0.12      0.53      0.19       330\n",
            "I-VGNN               0.00      0.00      0.00         1\n",
            "O                    1.00      1.00      1.00      1460\n",
            "\n",
            "avg / total          0.93      0.91      0.91     25963\n",
            "\n",
            "Accuracy: 0.9276277779917576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrWbErBM51T5",
        "outputId": "bfbfbf65-4969-41cb-a16f-841198ebcec7"
      },
      "source": [
        "model_new = keras.models.load_model(\"/content/drive/My Drive/explo/bhoj_explo_mapped.bin\", custom_objects={'CRF':CRF,'SeqSelfAttention':SeqSelfAttention, 'crf_loss':crf_loss,'crf_viterbi_accuracy':crf_viterbi_accuracy})\n",
        "# model_new = model_fast_map\n",
        "model_new.trainable = False\n",
        "base_input1_new = model_new.layers[1].input\n",
        "base_input2_new = model_new.layers[0].input\n",
        "base_input3_new = model_new.layers[3].input\n",
        "base_outputs_new = model_new.layers[-3].output\n",
        "\n",
        "o1 = (SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                       kernel_regularizer=keras.regularizers.l2(1e-5),\n",
        "                       bias_regularizer=keras.regularizers.l1(1e-5),\n",
        "                       attention_regularizer_weight=1e-5,\n",
        "                       name='Attention'))(base_outputs_new)\n",
        "crf = CRF(len(class_labels), sparse_target=True , name='crf')\n",
        "final_output_new = (crf)(o1)\n",
        "model1_new = Model(inputs = [base_input1_new,base_input2_new,base_input3_new], outputs = final_output_new)\n",
        "model1_new.compile('adam',loss=crf_loss, metrics=[crf_viterbi_accuracy])\n",
        "# X_train= numpy.array(X_train).reshape(len(X_train),max_len,max_len_char)\n",
        "# print(X_train.shape)\n",
        "\n",
        "# not a robo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Train on 3985 samples, validate on 443 samples\n",
            "Epoch 1/5\n",
            "3985/3985 [==============================] - 1222s 307ms/step - loss: 171.0196 - crf_viterbi_accuracy: 0.7533 - val_loss: 143.7047 - val_crf_viterbi_accuracy: 0.8308\n",
            "Epoch 2/5\n",
            "3985/3985 [==============================] - 1216s 305ms/step - loss: 170.5962 - crf_viterbi_accuracy: 0.8563 - val_loss: 143.6060 - val_crf_viterbi_accuracy: 0.8688\n",
            "Epoch 3/5\n",
            "3985/3985 [==============================] - 1200s 301ms/step - loss: 170.4955 - crf_viterbi_accuracy: 0.8926 - val_loss: 143.5740 - val_crf_viterbi_accuracy: 0.8906\n",
            "Epoch 4/5\n",
            "3985/3985 [==============================] - 1207s 303ms/step - loss: 170.4351 - crf_viterbi_accuracy: 0.9175 - val_loss: 143.5494 - val_crf_viterbi_accuracy: 0.9029\n",
            "Epoch 5/5\n",
            "3985/3985 [==============================] - 1211s 304ms/step - loss: 170.3943 - crf_viterbi_accuracy: 0.9329 - val_loss: 143.5214 - val_crf_viterbi_accuracy: 0.9035\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "char_in (InputLayer)            (None, 369, 23)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_in (InputLayer)            (None, 369)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 369, 23, 10)  1030        char_in[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pos_in (InputLayer)             (None, 369)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 369, 100)     1479700     word_in[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (None, 369, 100)     44400       time_distributed_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 369, 200)     5800        pos_in[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 369, 400)     0           embedding_4[0][0]                \n",
            "                                                                 time_distributed_4[0][0]         \n",
            "                                                                 embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 369, 400)     961600      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Attention (SeqSelfAttention)    (None, 369, 400)     160001      bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "crf (CRF)                       (None, 369, 28)      12068       Attention[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,664,599\n",
            "Trainable params: 2,664,599\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u443R696eLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e8fa8c-4c5f-43ef-809c-4d16c9f88cd6"
      },
      "source": [
        "l4=[0]*max_len_char\n",
        "l4=numpy.asarray(l4)\n",
        "y= model1_new.predict([X_test,np.array(X_char_test).reshape((len(X_char_test),max_len, max_len_char)),x_pos_test.reshape(len(x_pos_test),max_len)])\n",
        "y= y.argmax(-1)\n",
        "y_pred=[]\n",
        "test_y_true=[]\n",
        "ctest_y_pred=[]\n",
        "ctest_y_true=[]\n",
        "\n",
        "#removing padding from the validation\n",
        "for i in range(len(X_test)):\n",
        "    l=[]\n",
        "    l1=[]\n",
        "    for j in range(len(X_test[i])):\n",
        "        if (X_test[i][j]==0):\n",
        "            continue\n",
        "        l.append(y[i][j])\n",
        "        l1.append(y_test[i][j][0])\n",
        "        ctest_y_pred.append(y[i][j])\n",
        "        ctest_y_true.append(y_test[i][j][0])\n",
        "    y_pred.append(l)\n",
        "    test_y_true.append(l1)\n",
        "\n",
        "#converting to numpy array\n",
        "test_y_pred=numpy.asarray(y_pred)\n",
        "test_y_true=numpy.asarray(test_y_true)\n",
        "\n",
        "#writing output for validation\n",
        "f1=open(output_file,'w')\n",
        "for i in range(len(test_y_pred)):\n",
        "    for j in range(len(test_y_pred[i])):\n",
        "        s='|'+'\\t'+'NN'+'\\t'+str(class_labels[test_y_pred[i][j]])+'\\t'+str(class_labels[test_y_true[i][j]])\n",
        "        f1.write(s+'\\n')\n",
        "    f1.write('\\n')\n",
        "f1.flush()\n",
        "f1.close()\n",
        "print('\\n---- Result of Fine tuning BiLSTM-CRF ----\\n')\n",
        "classification_report(ctest_y_true, ctest_y_pred, class_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- Result of Fine tuning BiLSTM-CRF ----\n",
            "\n",
            "                   recall precision  f1-score   support\n",
            "\n",
            "B-BLK                0.76      0.83      0.80       355\n",
            "B-CCP                0.99      0.96      0.97       747\n",
            "B-JJP                0.14      0.40      0.20       160\n",
            "B-NEGP               0.33      0.59      0.42        73\n",
            "B-NNP                0.00      0.00      0.00         0\n",
            "B-NP                 0.96      0.94      0.95      6846\n",
            "B-NST                0.00      0.00      0.00         0\n",
            "B-PRP                0.00      0.00      0.00         0\n",
            "B-RBP                0.91      0.80      0.85        81\n",
            "B-RP                 0.00      0.00      0.00         0\n",
            "B-VGF                0.95      0.88      0.92      3205\n",
            "B-VGINF              0.00      0.00      0.00         9\n",
            "B-VGNF               0.22      0.64      0.33       394\n",
            "B-VGNN               0.00      0.00      0.00         4\n",
            "I-BLK                0.59      0.69      0.63        63\n",
            "I-CCP                0.00      0.00      0.00        11\n",
            "I-JJP                0.20      0.55      0.30       152\n",
            "I-NEGP               0.37      0.62      0.47        27\n",
            "I-NP                 0.96      0.96      0.96      9055\n",
            "I-NST                0.00      0.00      0.00         0\n",
            "I-PRP                0.00      0.00      0.00         0\n",
            "I-RBP                0.82      0.89      0.86        40\n",
            "I-RP                 0.00      0.00      0.00         0\n",
            "I-VGF                0.97      0.90      0.93      2940\n",
            "I-VGINF              0.00      0.00      0.00        10\n",
            "I-VGNF               0.33      0.67      0.44       330\n",
            "I-VGNN               0.00      0.00      0.00         1\n",
            "O                    0.99      0.96      0.97      1460\n",
            "\n",
            "avg / total          0.93      0.92      0.92     25963\n",
            "\n",
            "Accuracy: 0.9285136540461426\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRRlRhE8IjQr"
      },
      "source": [
        "model1_new.save('/content/drive/My Drive/explo/bhoj_w2v_fine.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYeFujPLezM3",
        "outputId": "359f3ab4-f729-4c59-c56e-ae8219017e66"
      },
      "source": [
        "f = open ('/content/drive/My Drive/explo/fastemb_bhoj.vec','r')\n",
        "embeddings_index={}\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "embedding_matrix = np.zeros((len(vocab), 100))\n",
        "i = 0\n",
        "for word in vocab:\n",
        "    # values = line.split()\n",
        "    # word = values[0]\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    i=i+1\n",
        "\n",
        "from keras import backend as K\n",
        "# word embedding\n",
        "\n",
        "word_in=Input(shape=(X_train.shape[1],),name='word_in')\n",
        "emb_word=(Embedding(len(vocab), EMBED_DIM//4, weights = [embedding_matrix], mask_zero=True))(word_in)\n",
        "\n",
        "print(X_train.shape[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lQNV3JEe4_D",
        "outputId": "c17463c7-8400-4273-ca64-ef43d933d7de"
      },
      "source": [
        "# charcter embedding\n",
        "char_in = Input(shape=(X_char_train.shape[1],X_char_train.shape[2]), name='char_in')\n",
        "emb_char=(TimeDistributed(Embedding(input_dim=len(characters),output_dim=10,input_length=max_len_char,mask_zero=True)))(char_in)\n",
        "char_enc=(TimeDistributed(LSTM(units=EMBED_DIM//4,return_sequences=False)))(emb_char)\n",
        "\n",
        "# pos_tag embedding \n",
        "pos_in=Input(shape=(x_pos_train.shape[1],),name='pos_in')\n",
        "pos_word=(Embedding(len(pos_tags), EMBED_DIM//2, mask_zero=True))(pos_in)\n",
        "\n",
        "# concatenating them word+char+pos\n",
        "x = concatenate([emb_word, char_enc, pos_word])\n",
        "\n",
        "#passing to Bi-LSTM layer\n",
        "o=(Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True, dropout=0.2)))(x)\n",
        "#print(o)\n",
        "\n",
        "#Self attention layer\n",
        "o = (SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                       kernel_regularizer=keras.regularizers.l2(1e-4),\n",
        "                       bias_regularizer=keras.regularizers.l1(1e-4),\n",
        "                       attention_regularizer_weight=1e-4,\n",
        "                       name='Attention'))(o)\n",
        "\n",
        "#CRF layer\n",
        "crf = CRF(len(class_labels), sparse_target=True,name='crf')\n",
        "o=(crf)(o)\n",
        "model_fast=Model([word_in,char_in,pos_in],o)\n",
        "model_fast.compile('adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAEgzeVde7fe",
        "outputId": "d27372b7-8f10-4e50-f685-2dd6b298c2b0"
      },
      "source": [
        "EPOCHS = 5\n",
        "model_fast.fit([X_train,np.array(X_char_train).reshape(len(X_char_train),max_len,max_len_char),x_pos_train.reshape(len(x_pos_train),max_len)],np.array(y_train).reshape(len(y_train), max_len, 1),batch_size=32, epochs=EPOCHS, validation_split=0.1, verbose=1)\n",
        "model_fast.save('/content/drive/My Drive/explo/bhoj_explo_fast.bin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3985 samples, validate on 443 samples\n",
            "Epoch 1/5\n",
            "3985/3985 [==============================] - 1354s 340ms/step - loss: 172.3551 - crf_viterbi_accuracy: 0.4976 - val_loss: 144.4565 - val_crf_viterbi_accuracy: 0.8038\n",
            "Epoch 2/5\n",
            "3985/3985 [==============================] - 1325s 332ms/step - loss: 171.5785 - crf_viterbi_accuracy: 0.8161 - val_loss: 144.1963 - val_crf_viterbi_accuracy: 0.8684\n",
            "Epoch 3/5\n",
            "3985/3985 [==============================] - 1328s 333ms/step - loss: 171.2820 - crf_viterbi_accuracy: 0.8856 - val_loss: 143.9452 - val_crf_viterbi_accuracy: 0.9130\n",
            "Epoch 4/5\n",
            "3985/3985 [==============================] - 1314s 330ms/step - loss: 171.1433 - crf_viterbi_accuracy: 0.9152 - val_loss: 143.8840 - val_crf_viterbi_accuracy: 0.9235\n",
            "Epoch 5/5\n",
            "3985/3985 [==============================] - 1296s 325ms/step - loss: 171.1043 - crf_viterbi_accuracy: 0.9278 - val_loss: 143.8726 - val_crf_viterbi_accuracy: 0.9284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF5a5RSgfAMf",
        "outputId": "efe9e386-38d1-4634-e116-021b664e17ac"
      },
      "source": [
        "# #validation\n",
        "# model_json = model.to_json()\n",
        "# with open(\"model_bhoj_mapped.json\", \"w\") as json_file:\n",
        "#     json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "# model1_new.save_weights(\"/content/drive/My Drive/explo/model_bhoj_fine.h5\")\n",
        "# print(\"Saved model to disk\")\n",
        "l4=[0]*max_len_char\n",
        "l4=numpy.asarray(l4)\n",
        "y= model_fast.predict([X_test,np.array(X_char_test).reshape((len(X_char_test),max_len, max_len_char)),x_pos_test.reshape(len(x_pos_test),max_len)])\n",
        "y= y.argmax(-1)\n",
        "y_pred=[]\n",
        "test_y_true=[]\n",
        "ctest_y_pred=[]\n",
        "ctest_y_true=[]\n",
        "\n",
        "#removing padding from the validation\n",
        "for i in range(len(X_test)):\n",
        "    l=[]\n",
        "    l1=[]\n",
        "    for j in range(len(X_test[i])):\n",
        "        if (X_test[i][j]==0):\n",
        "            continue\n",
        "        l.append(y[i][j])\n",
        "        l1.append(y_test[i][j][0])\n",
        "        ctest_y_pred.append(y[i][j])\n",
        "        ctest_y_true.append(y_test[i][j][0])\n",
        "    y_pred.append(l)\n",
        "    test_y_true.append(l1)\n",
        "\n",
        "#converting to numpy array\n",
        "test_y_pred=numpy.asarray(y_pred)\n",
        "test_y_true=numpy.asarray(test_y_true)\n",
        "\n",
        "#writing output for validation\n",
        "f1=open(output_file,'w')\n",
        "for i in range(len(test_y_pred)):\n",
        "    for j in range(len(test_y_pred[i])):\n",
        "        s='|'+'\\t'+'NN'+'\\t'+str(class_labels[test_y_pred[i][j]])+'\\t'+str(class_labels[test_y_true[i][j]])\n",
        "        f1.write(s+'\\n')\n",
        "    f1.write('\\n')\n",
        "f1.flush()\n",
        "f1.close()\n",
        "print('\\n---- Result of BiLSTM-CRF ----\\n')\n",
        "classification_report(ctest_y_true, ctest_y_pred, class_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- Result of BiLSTM-CRF ----\n",
            "\n",
            "                   recall precision  f1-score   support\n",
            "\n",
            "B-BLK                0.70      0.81      0.75       355\n",
            "B-CCP                0.99      0.99      0.99       747\n",
            "B-JJP                0.00      0.00      0.00       160\n",
            "B-NEGP               0.41      0.77      0.54        73\n",
            "B-NNP                0.00      0.00      0.00         0\n",
            "B-NP                 0.96      0.94      0.95      6846\n",
            "B-NST                0.00      0.00      0.00         0\n",
            "B-PRP                0.00      0.00      0.00         0\n",
            "B-RBP                0.73      0.80      0.76        81\n",
            "B-RP                 0.00      0.00      0.00         0\n",
            "B-VGF                0.98      0.87      0.92      3205\n",
            "B-VGINF              0.00      0.00      0.00         9\n",
            "B-VGNF               0.02      0.58      0.03       394\n",
            "B-VGNN               0.00      0.00      0.00         4\n",
            "I-BLK                0.33      0.81      0.47        63\n",
            "I-CCP                0.00      0.00      0.00        11\n",
            "I-JJP                0.00      0.00      0.00       152\n",
            "I-NEGP               0.00      0.00      0.00        27\n",
            "I-NP                 0.98      0.96      0.97      9055\n",
            "I-NST                0.00      0.00      0.00         0\n",
            "I-PRP                0.00      0.00      0.00         0\n",
            "I-RBP                0.53      1.00      0.69        40\n",
            "I-RP                 0.00      0.00      0.00         0\n",
            "I-VGF                0.99      0.86      0.92      2940\n",
            "I-VGINF              0.00      0.00      0.00        10\n",
            "I-VGNF               0.02      0.37      0.04       330\n",
            "I-VGNN               0.00      0.00      0.00         1\n",
            "O                    1.00      1.00      1.00      1460\n",
            "\n",
            "avg / total          0.93      0.90      0.91     25963\n",
            "\n",
            "Accuracy: 0.9277433270423294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1z--UPP6Es5"
      },
      "source": [
        "f = open ('/content/drive/My Drive/explo/TRG_MAPPED.EMB','r')\n",
        "embeddings_index={}\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "embedding_matrix = np.zeros((len(vocab), 100))\n",
        "i = 0\n",
        "for word in vocab:\n",
        "    # values = line.split()\n",
        "    # word = values[0]\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    i=i+1\n",
        "\n",
        "from keras import backend as K\n",
        "# word embedding\n",
        "\n",
        "word_in=Input(shape=(X_train.shape[1],),name='word_in')\n",
        "emb_word=(Embedding(len(vocab), EMBED_DIM//4, weights = [embedding_matrix], mask_zero=True))(word_in)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuWidGPP6r25"
      },
      "source": [
        "# charcter embedding\n",
        "char_in = Input(shape=(X_char_train.shape[1],X_char_train.shape[2]), name='char_in')\n",
        "emb_char=(TimeDistributed(Embedding(input_dim=len(characters),output_dim=10,input_length=max_len_char,mask_zero=True)))(char_in)\n",
        "char_enc=(TimeDistributed(LSTM(units=EMBED_DIM//4,return_sequences=False)))(emb_char)\n",
        "\n",
        "# pos_tag embedding \n",
        "pos_in=Input(shape=(x_pos_train.shape[1],),name='pos_in')\n",
        "pos_word=(Embedding(len(pos_tags), EMBED_DIM//2, mask_zero=True))(pos_in)\n",
        "\n",
        "# concatenating them word+char+pos\n",
        "x = concatenate([emb_word, char_enc, pos_word])\n",
        "\n",
        "#passing to Bi-LSTM layer\n",
        "o=(Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True, dropout=0.2)))(x)\n",
        "#print(o)\n",
        "\n",
        "#Self attention layer\n",
        "o = (SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                       kernel_regularizer=keras.regularizers.l2(1e-4),\n",
        "                       bias_regularizer=keras.regularizers.l1(1e-4),\n",
        "                       attention_regularizer_weight=1e-4,\n",
        "                       name='Attention'))(o)\n",
        "\n",
        "#CRF layer\n",
        "crf = CRF(len(class_labels), sparse_target=True,name='crf')\n",
        "o=(crf)(o)\n",
        "model_fast=Model([word_in,char_in,pos_in],o)\n",
        "model_fast.compile('adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjm4LLqw5ysn",
        "outputId": "fa1626cc-e60e-45a4-c5c3-6bea07c6fffa"
      },
      "source": [
        "model_fast_map=Model([word_in,char_in,pos_in],o)\n",
        "model_fast_map.compile('adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])\n",
        "\n",
        "EPOCHS = 5\n",
        "model_fast_map.fit([X_train,np.array(X_char_train).reshape(len(X_char_train),max_len,max_len_char),x_pos_train.reshape(len(x_pos_train),max_len)],np.array(y_train).reshape(len(y_train), max_len, 1),batch_size=32, epochs=EPOCHS, validation_split=0.1, verbose=1)\n",
        "model_fast_map.save('/content/drive/My Drive/explo/bhoj_explo_fast_map.bin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3985 samples, validate on 443 samples\n",
            "Epoch 1/5\n",
            "3985/3985 [==============================] - 1307s 328ms/step - loss: 172.6132 - crf_viterbi_accuracy: 0.3940 - val_loss: 144.6253 - val_crf_viterbi_accuracy: 0.6547\n",
            "Epoch 2/5\n",
            "3985/3985 [==============================] - 1302s 327ms/step - loss: 171.5724 - crf_viterbi_accuracy: 0.7886 - val_loss: 144.1506 - val_crf_viterbi_accuracy: 0.8895\n",
            "Epoch 3/5\n",
            "3985/3985 [==============================] - 1292s 324ms/step - loss: 171.1874 - crf_viterbi_accuracy: 0.9097 - val_loss: 143.8693 - val_crf_viterbi_accuracy: 0.9220\n",
            "Epoch 4/5\n",
            "3985/3985 [==============================] - 1293s 324ms/step - loss: 171.0891 - crf_viterbi_accuracy: 0.9272 - val_loss: 143.8387 - val_crf_viterbi_accuracy: 0.9258\n",
            "Epoch 5/5\n",
            "3985/3985 [==============================] - 1282s 322ms/step - loss: 171.0400 - crf_viterbi_accuracy: 0.9411 - val_loss: 143.8105 - val_crf_viterbi_accuracy: 0.9291\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twEKKovIThr-",
        "outputId": "a7be83ee-3af3-45d1-a108-a944f0e4d1ff"
      },
      "source": [
        "# #validation\n",
        "# model_json = model.to_json()\n",
        "# with open(\"model_bhoj_mapped.json\", \"w\") as json_file:\n",
        "#     json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "# model1_new.save_weights(\"/content/drive/My Drive/explo/model_bhoj_fine.h5\")\n",
        "# print(\"Saved model to disk\")\n",
        "l4=[0]*max_len_char\n",
        "l4=numpy.asarray(l4)\n",
        "y= model_fast_map.predict([X_test,np.array(X_char_test).reshape((len(X_char_test),max_len, max_len_char)),x_pos_test.reshape(len(x_pos_test),max_len)])\n",
        "y= y.argmax(-1)\n",
        "y_pred=[]\n",
        "test_y_true=[]\n",
        "ctest_y_pred=[]\n",
        "ctest_y_true=[]\n",
        "\n",
        "#removing padding from the validation\n",
        "for i in range(len(X_test)):\n",
        "    l=[]\n",
        "    l1=[]\n",
        "    for j in range(len(X_test[i])):\n",
        "        if (X_test[i][j]==0):\n",
        "            continue\n",
        "        l.append(y[i][j])\n",
        "        l1.append(y_test[i][j][0])\n",
        "        ctest_y_pred.append(y[i][j])\n",
        "        ctest_y_true.append(y_test[i][j][0])\n",
        "    y_pred.append(l)\n",
        "    test_y_true.append(l1)\n",
        "\n",
        "#converting to numpy array\n",
        "test_y_pred=numpy.asarray(y_pred)\n",
        "test_y_true=numpy.asarray(test_y_true)\n",
        "\n",
        "#writing output for validation\n",
        "f1=open(output_file,'w')\n",
        "for i in range(len(test_y_pred)):\n",
        "    for j in range(len(test_y_pred[i])):\n",
        "        s='|'+'\\t'+'NN'+'\\t'+str(class_labels[test_y_pred[i][j]])+'\\t'+str(class_labels[test_y_true[i][j]])\n",
        "        f1.write(s+'\\n')\n",
        "    f1.write('\\n')\n",
        "f1.flush()\n",
        "f1.close()\n",
        "print('\\n---- Result of BiLSTM-CRF ----\\n')\n",
        "classification_report(ctest_y_true, ctest_y_pred, class_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- Result of BiLSTM-CRF ----\n",
            "\n",
            "                   recall precision  f1-score   support\n",
            "\n",
            "B-BLK                0.76      0.78      0.77       355\n",
            "B-CCP                0.99      0.98      0.99       747\n",
            "B-JJP                0.06      0.64      0.10       160\n",
            "B-NEGP               0.56      0.84      0.67        73\n",
            "B-NNP                0.00      0.00      0.00         0\n",
            "B-NP                 0.95      0.95      0.95      6846\n",
            "B-NST                0.00      0.00      0.00         0\n",
            "B-PRP                0.00      0.00      0.00         0\n",
            "B-RBP                0.85      0.87      0.86        81\n",
            "B-RP                 0.00      0.00      0.00         0\n",
            "B-VGF                0.97      0.89      0.93      3205\n",
            "B-VGINF              0.00      0.00      0.00         9\n",
            "B-VGNF               0.24      0.63      0.35       394\n",
            "B-VGNN               0.00      0.00      0.00         4\n",
            "I-BLK                0.38      0.77      0.51        63\n",
            "I-CCP                0.00      0.00      0.00        11\n",
            "I-JJP                0.20      0.52      0.29       152\n",
            "I-NEGP               0.00      0.00      0.00        27\n",
            "I-NP                 0.98      0.95      0.97      9055\n",
            "I-NST                0.00      0.00      0.00         0\n",
            "I-PRP                0.00      0.00      0.00         0\n",
            "I-RBP                0.93      0.90      0.91        40\n",
            "I-RP                 0.00      0.00      0.00         0\n",
            "I-VGF                0.97      0.89      0.93      2940\n",
            "I-VGINF              0.00      0.00      0.00        10\n",
            "I-VGNF               0.33      0.62      0.43       330\n",
            "I-VGNN               0.00      0.00      0.00         1\n",
            "O                    1.00      0.98      0.99      1460\n",
            "\n",
            "avg / total          0.93      0.92      0.92     25963\n",
            "\n",
            "Accuracy: 0.9320956746138735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5coaDZSVARn",
        "outputId": "95310b9a-f924-4191-eeb4-6db89d82a126"
      },
      "source": [
        "# model_new = keras.models.load_model(\"/content/drive/My Drive/explo/mai_explo_transfer.bin\", custom_objects={'CRF':CRF,'SeqSelfAttention':SeqSelfAttention, 'crf_loss':crf_loss,'crf_viterbi_accuracy':crf_viterbi_accuracy})\n",
        "model_new = model_fast_map\n",
        "model_new.trainable = False\n",
        "base_input1_new = model_new.layers[1].input\n",
        "base_input2_new = model_new.layers[0].input\n",
        "base_input3_new = model_new.layers[3].input\n",
        "base_outputs_new = model_new.layers[-3].output\n",
        "\n",
        "o1 = (SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                       kernel_regularizer=keras.regularizers.l2(1e-4),\n",
        "                       bias_regularizer=keras.regularizers.l1(1e-5),\n",
        "                       attention_regularizer_weight=1e-4,\n",
        "                       name='Attention'))(base_outputs_new)\n",
        "crf = CRF(len(class_labels), sparse_target=True , name='crf')\n",
        "final_output_new = (crf)(o1)\n",
        "model1_new = Model(inputs = [base_input1_new,base_input2_new,base_input3_new], outputs = final_output_new)\n",
        "model1_new.compile('adam',loss=crf_loss, metrics=[crf_viterbi_accuracy])\n",
        "# X_train= numpy.array(X_train).reshape(len(X_train),max_len,max_len_char)\n",
        "# print(X_train.shape)\n",
        "EPOCHS = 5\n",
        "model1_new.fit([X_train,np.array(X_char_train).reshape(len(X_char_train),max_len,max_len_char),\n",
        "           x_pos_train.reshape(len(x_pos_train),max_len)],np.array(y_train).reshape(len(y_train), max_len, 1),\n",
        "           batch_size=32, epochs=EPOCHS, validation_split=0.1, verbose=1)\n",
        "model1_new.save('/content/drive/My Drive/explo/bhoj_fast_map_fine.bin')\n",
        "print(model1_new.summary())\n",
        "\n",
        "# not a robo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3985 samples, validate on 443 samples\n",
            "Epoch 1/5\n",
            "3985/3985 [==============================] - 1304s 327ms/step - loss: 172.0453 - crf_viterbi_accuracy: 0.6708 - val_loss: 144.1404 - val_crf_viterbi_accuracy: 0.8610\n",
            "Epoch 2/5\n",
            "3985/3985 [==============================] - 1284s 322ms/step - loss: 171.2332 - crf_viterbi_accuracy: 0.9176 - val_loss: 143.9125 - val_crf_viterbi_accuracy: 0.9186\n",
            "Epoch 3/5\n",
            "3985/3985 [==============================] - 1286s 323ms/step - loss: 171.0771 - crf_viterbi_accuracy: 0.9402 - val_loss: 143.8711 - val_crf_viterbi_accuracy: 0.9261\n",
            "Epoch 4/5\n",
            "3985/3985 [==============================] - 1302s 327ms/step - loss: 171.0225 - crf_viterbi_accuracy: 0.9561 - val_loss: 143.8427 - val_crf_viterbi_accuracy: 0.9314\n",
            "Epoch 5/5\n",
            "3985/3985 [==============================] - 1291s 324ms/step - loss: 170.9918 - crf_viterbi_accuracy: 0.9666 - val_loss: 143.8377 - val_crf_viterbi_accuracy: 0.9344\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "char_in (InputLayer)            (None, 369, 23)      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_in (InputLayer)            (None, 369)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 369, 23, 10)  1030        char_in[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pos_in (InputLayer)             (None, 369)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 369, 100)     1479700     word_in[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (None, 369, 100)     44400       time_distributed_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 369, 200)     5800        pos_in[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 369, 400)     0           embedding_4[0][0]                \n",
            "                                                                 time_distributed_4[0][0]         \n",
            "                                                                 embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 369, 400)     961600      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Attention (SeqSelfAttention)    (None, 369, 400)     160001      bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "crf (CRF)                       (None, 369, 28)      12068       Attention[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,664,599\n",
            "Trainable params: 2,664,599\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9upeT-LrqwP",
        "outputId": "cca96d13-20c3-42e2-c49e-cd0375d87d7c"
      },
      "source": [
        "l4=[0]*max_len_char\n",
        "l4=numpy.asarray(l4)\n",
        "y= model1_new.predict([X_test,np.array(X_char_test).reshape((len(X_char_test),max_len, max_len_char)),x_pos_test.reshape(len(x_pos_test),max_len)])\n",
        "y= y.argmax(-1)\n",
        "y_pred=[]\n",
        "test_y_true=[]\n",
        "ctest_y_pred=[]\n",
        "ctest_y_true=[]\n",
        "\n",
        "#removing padding from the validation\n",
        "for i in range(len(X_test)):\n",
        "    l=[]\n",
        "    l1=[]\n",
        "    for j in range(len(X_test[i])):\n",
        "        if (X_test[i][j]==0):\n",
        "            continue\n",
        "        l.append(y[i][j])\n",
        "        l1.append(y_test[i][j][0])\n",
        "        ctest_y_pred.append(y[i][j])\n",
        "        ctest_y_true.append(y_test[i][j][0])\n",
        "    y_pred.append(l)\n",
        "    test_y_true.append(l1)\n",
        "\n",
        "#converting to numpy array\n",
        "test_y_pred=numpy.asarray(y_pred)\n",
        "test_y_true=numpy.asarray(test_y_true)\n",
        "\n",
        "#writing output for validation\n",
        "f1=open(output_file,'w')\n",
        "for i in range(len(test_y_pred)):\n",
        "    for j in range(len(test_y_pred[i])):\n",
        "        s='|'+'\\t'+'NN'+'\\t'+str(class_labels[test_y_pred[i][j]])+'\\t'+str(class_labels[test_y_true[i][j]])\n",
        "        f1.write(s+'\\n')\n",
        "    f1.write('\\n')\n",
        "f1.flush()\n",
        "f1.close()\n",
        "print('\\n---- Result of Fine tuning BiLSTM-CRF ----\\n')\n",
        "classification_report(ctest_y_true, ctest_y_pred, class_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- Result of Fine tuning BiLSTM-CRF ----\n",
            "\n",
            "                   recall precision  f1-score   support\n",
            "\n",
            "B-BLK                0.73      0.78      0.75       355\n",
            "B-CCP                0.98      0.97      0.97       747\n",
            "B-JJP                0.28      0.56      0.38       160\n",
            "B-NEGP               0.63      0.78      0.70        73\n",
            "B-NNP                0.00      0.00      0.00         0\n",
            "B-NP                 0.96      0.95      0.95      6846\n",
            "B-NST                0.00      0.00      0.00         0\n",
            "B-PRP                0.00      0.00      0.00         0\n",
            "B-RBP                0.86      0.88      0.87        81\n",
            "B-RP                 0.00      0.00      0.00         0\n",
            "B-VGF                0.96      0.89      0.93      3205\n",
            "B-VGINF              0.00      0.00      0.00         9\n",
            "B-VGNF               0.23      0.57      0.33       394\n",
            "B-VGNN               0.00      0.00      0.00         4\n",
            "I-BLK                0.60      0.59      0.60        63\n",
            "I-CCP                0.00      0.00      0.00        11\n",
            "I-JJP                0.35      0.55      0.43       152\n",
            "I-NEGP               0.37      0.91      0.53        27\n",
            "I-NP                 0.97      0.96      0.97      9055\n",
            "I-NST                0.00      0.00      0.00         0\n",
            "I-PRP                0.00      0.00      0.00         0\n",
            "I-RBP                0.95      0.93      0.94        40\n",
            "I-RP                 0.00      0.00      0.00         0\n",
            "I-VGF                0.95      0.92      0.93      2940\n",
            "I-VGINF              0.00      0.00      0.00        10\n",
            "I-VGNF               0.46      0.61      0.53       330\n",
            "I-VGNN               0.00      0.00      0.00         1\n",
            "O                    1.00      1.00      1.00      1460\n",
            "\n",
            "avg / total          0.93      0.93      0.93     25963\n",
            "\n",
            "Accuracy: 0.9340985248237877\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}