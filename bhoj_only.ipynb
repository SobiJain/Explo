{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bhoj_only.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcDIkvbwUwF2",
        "outputId": "b8098d82-611b-4485-99f8-cb01cdd43e24"
      },
      "source": [
        "!pip install --user keras\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "!git clone https://github.com/ziadloo/attention_keras.git\n",
        "!pip install keras-attention\n",
        "!pip install keras-self-attention\n",
        "!pip install keras==2.2.4        \n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "!pip install tensorflow==1.14.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.15.0)\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-zjdzzl5k\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-zjdzzl5k\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=680a06ad56df5e1e8795fd85c366e3ce2dcb43d56a0e2627f0422b514772d8c3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zt_i7whi/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n",
            "Cloning into 'attention_keras'...\n",
            "remote: Enumerating objects: 199, done.\u001b[K\n",
            "remote: Total 199 (delta 0), reused 0 (delta 0), pack-reused 199\u001b[K\n",
            "Receiving objects: 100% (199/199), 47.20 MiB | 28.58 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n",
            "Collecting keras-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/9a/e9/1373dff1b2dd7cb45208d224c84d7a2bddcf0519f01d98f07d8ce15dc4be/keras_attention-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-attention) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-attention) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-attention) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-attention) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-attention) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-attention) (1.15.0)\n",
            "Installing collected packages: keras-attention\n",
            "Successfully installed keras-attention-1.0.0\n",
            "Collecting keras-self-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/34/e21dc6adcdab2be03781bde78c6c5d2b2136d35a1dd3e692d7e160ba062a/keras-self-attention-0.49.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras->keras-self-attention) (1.15.0)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.49.0-cp37-none-any.whl size=19468 sha256=0f854f75f86885897b3ec817482a99935f31125bed25c68cf9c53eb3edd0fa29\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/9d/c5/26693a5092d9313daeae94db04818fc0a2b7a48ea381989f34\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.49.0\n",
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Installing collected packages: keras-applications, keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.2.4 keras-applications-1.0.8\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-z5rfpslr\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-z5rfpslr\n",
            "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101065 sha256=fd9e80608bf9b3524f23552f8b1f36122b09d0e09da27956a0580bb1ee9bd9c5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mgjuqieo/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/28/96efba1a516cdacc2e2d6d081f699c001d414cc8ca3250e6d59ae657eb2b/tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3MB 61kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.32.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.12.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (54.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.4.3)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghYPqBrqWSsh",
        "outputId": "a9254f3e-d673-43a0-8e57-7636cba38d2b"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import numpy\n",
        "from collections import Counter\n",
        "from keras.models import *\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dropout, Dense,concatenate\n",
        "from keras_contrib.layers import CRF\n",
        "from keras_contrib.losses import crf_loss\n",
        "from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "from keras_contrib.datasets import conll2000\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from collections import Counter\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJYBqsQkWV1y"
      },
      "source": [
        "\n",
        "EMBED_DIM = 400\n",
        "BiRNN_UNITS = 400\n",
        "max_len_char=-1\n",
        "max_len=None\n",
        "chunking_file_path = '/content/drive/My Drive/explo/bhoj_chunk.txt'\n",
        "output_file = 'output.txt'\n",
        "def load_data(chunking_file_path, min_freq=1):\n",
        "\n",
        "    file_train = _parse_data(open(chunking_file_path))\n",
        "    print (len(file_train))\n",
        "    count1=0\n",
        "    for i in file_train:\n",
        "        count1=count1+len(i)\n",
        "    print (count1)\n",
        "    word_counts = Counter(row[0].lower() for sample in file_train for row in sample if len(row)>=3)\n",
        "    vocab = ['<pad>', '<unk>']\n",
        "    vocab += [w for w, f in iter(word_counts.items()) if f >= min_freq]\n",
        "    pos_tags = sorted(list(set(row[1] for sample in file_train for row in sample if len(row)>=3)))\n",
        "    chunk_tags = sorted(list(set(row[2] for sample in file_train for row in sample if len(row)>=3)))\n",
        "    characters = sorted(list(set(i for sample in file_train for row in sample if len(row)>=3 for i in row[0])))\n",
        "    characters.insert(0,'<unk>')\n",
        "    characters.insert(0,'<pad>')\n",
        "    pos_tags.insert(0,'<unk>')\n",
        "    pos_tags.insert(0,'<pad>')\n",
        "    global max_len_char\n",
        "    for sample in file_train:\n",
        "        for row in sample:\n",
        "            if len(row) >= 3:\n",
        "                max_len_char=max(max_len_char,len(row[0]))\n",
        "    train = _process_data(file_train, vocab, pos_tags, chunk_tags,characters)\n",
        "    return train, (vocab, pos_tags, chunk_tags,characters)\n",
        "def _parse_data(fh):\n",
        "    string = fh.read()\n",
        "    # print(string)\n",
        "    data = []\n",
        "    for sample in string.strip().split('\\n\\n'):\n",
        "        data.append([row.split() for row in sample.split('\\n')])\n",
        "    fh.close()\n",
        "    return data\n",
        "def pad_words(l,max_len_char):\n",
        "    length=len(l)\n",
        "    l1=[0 for i in range(max_len_char-length)]\n",
        "    l=l1+l\n",
        "    return l\n",
        "def _process_data(data, vocab, pos_tags, chunk_tags, characters, onehot=False):\n",
        "    global max_len\n",
        "    if max_len is None:\n",
        "        max_len = max(len(s) for s in data)\n",
        "    word2idx = dict((w, i) for i, w in enumerate(vocab))\n",
        "    x = [[word2idx.get(w[0].lower(), 1) for w in s if len(w)>=3] for s in data]\n",
        "    y_pos = [[pos_tags.index(w[1]) for w in s if len(w)>=3] for s in data]\n",
        "    y_chunk = [[chunk_tags.index(w[2]) for w in s if len(w)>=3] for s in data]\n",
        "    defaultvalue=[0 for i in range(max_len_char)]\n",
        "    x = pad_sequences(x, max_len)\n",
        "    y_pos = pad_sequences(y_pos, max_len, value=0)\n",
        "    y_chunk = pad_sequences(y_chunk, max_len, value=0)\n",
        "\n",
        "    if onehot:\n",
        "        y_pos = numpy.eye(len(pos_tags), dtype='float32')[y]\n",
        "        y_chunk = numpy.eye(len(chunk_tags), dtype='float32')[y]\n",
        "    else:\n",
        "        y_pos = numpy.expand_dims(y_pos, 2)\n",
        "        y_chunk = numpy.expand_dims(y_chunk, 2)\n",
        "    return x, y_pos, y_chunk\n",
        "def tocharacter(characters,vocab,X_train):\n",
        "    ''' Function to create word embedding into character embedding'''\n",
        "    char2idx= dict((w,i) for i,w in enumerate(characters))\n",
        "    idx2word= dict((i, w) for i, w in enumerate(vocab)) \n",
        "    l=[]\n",
        "    for s in X_train:\n",
        "        l1=[]\n",
        "        for w in s:\n",
        "            if (idx2word[w]=='<pad>'):\n",
        "                l1.append([0]*max_len_char)\n",
        "                continue\n",
        "            if (idx2word[w]=='<unk>'):\n",
        "                l1.append([1]*max_len_char)\n",
        "                continue\n",
        "            l2=[]\n",
        "            for c in idx2word[w]:\n",
        "                l2.append(char2idx.get(c,1))\n",
        "            l2=pad_words(l2,max_len_char)\n",
        "            l1.append(l2)\n",
        "        l.append(l1)\n",
        "    return numpy.asarray(l)\n",
        "def classification_report(y_true, y_pred, labels):\n",
        "    '''Similar to the one in sklearn.metrics,\n",
        "    reports per classs recall, precision and F1 score'''\n",
        "    y_true = numpy.asarray(y_true).ravel()\n",
        "    y_pred = numpy.asarray(y_pred).ravel()\n",
        "    corrects = Counter(yt for yt, yp in zip(y_true, y_pred) if yt == yp)\n",
        "    y_true_counts = Counter(y_true)\n",
        "    y_pred_counts = Counter(y_pred)\n",
        "    report = ((lab,  # label\n",
        "               corrects[i] / max(1, y_true_counts[i]),  # recall\n",
        "               corrects[i] / max(1, y_pred_counts[i]),  # precision\n",
        "               y_true_counts[i]  # support\n",
        "               ) for i, lab in enumerate(labels))\n",
        "    report = [(l, r, p, 2 * r * p / max(1e-9, r + p), s) for l, r, p, s in report]\n",
        "\n",
        "    print('{:<15}{:>10}{:>10}{:>10}{:>10}\\n'.format('',\n",
        "                                                    'recall',\n",
        "                                                    'precision',\n",
        "                                                    'f1-score',\n",
        "                                                    'support'))\n",
        "    formatter = '{:<15}{:>10.2f}{:>10.2f}{:>10.2f}{:>10d}'.format\n",
        "    for r in report:\n",
        "        print(formatter(*r))\n",
        "    print('')\n",
        "    report2 = list(zip(*[(r * s, p * s, f1 * s) for l, r, p, f1, s in report]))\n",
        "    N = len(y_true)\n",
        "    print(formatter('avg / total',\n",
        "                    sum(report2[0]) / N,\n",
        "                    sum(report2[1]) / N,\n",
        "                    sum(report2[2]) / N, N) + '\\n')\n",
        "    actual = Counter(y_true)\n",
        "    del actual[-1]\n",
        "    accuracy = sum(corrects.values()) / sum(actual.values())\n",
        "    print('Accuracy:', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnptFQtbWy1D",
        "outputId": "2ab766a8-27f0-4f6b-ac4f-933aba0c27c3"
      },
      "source": [
        "train, voc = load_data(chunking_file_path)\n",
        "(X,x_pos,y) = train\n",
        "(vocab, pos_tags, class_labels,characters) = voc\n",
        "f2=open(\"vocabs.txt\",\"w\")\n",
        "f2.write(\" \".join(vocab)+'\\n')\n",
        "f2.write(\" \".join(characters)+'\\n')\n",
        "f2.write(\" \".join(pos_tags)+'\\n')\n",
        "f2.write(\" \".join(class_labels)+'\\n')\n",
        "f2.flush()\n",
        "f2.close()\n",
        "X_char=tocharacter(characters,vocab,X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=2018)\n",
        "X_char_train, X_char_test, __ , __ = train_test_split(X_char, y, test_size=0.3,random_state=2018)\n",
        "x_pos_train,x_pos_test,__, ___ = train_test_split(x_pos, y, test_size=0.3,random_state=2018)\n",
        "print('==== training BiLSTM-CRF ====')\n",
        "print(max_len_char)\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6327\n",
            "87729\n",
            "==== training BiLSTM-CRF ====\n",
            "23\n",
            "369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFRjZNIeGUXu",
        "outputId": "303a1879-c5ab-4f72-e7ea-c5c9cce60572"
      },
      "source": [
        "from keras import backend as K\n",
        "# word embedding\n",
        "word_in=Input(shape=(X_train.shape[1],),name='word_in')\n",
        "emb_word=(Embedding(len(vocab), EMBED_DIM//4, mask_zero=True))(word_in)\n",
        "\n",
        "print(X_train.shape[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVbxPsz5W3dY"
      },
      "source": [
        "# charcter embedding\n",
        "char_in = Input(shape=(X_char_train.shape[1],X_char_train.shape[2]), name='char_in')\n",
        "emb_char=(TimeDistributed(Embedding(input_dim=len(characters),output_dim=10,input_length=max_len_char,mask_zero=True)))(char_in)\n",
        "char_enc=(TimeDistributed(LSTM(units=EMBED_DIM//4,return_sequences=False)))(emb_char)\n",
        "\n",
        "# pos_tag embedding \n",
        "pos_in=Input(shape=(x_pos_train.shape[1],),name='pos_in')\n",
        "pos_word=(Embedding(len(pos_tags), EMBED_DIM//2, mask_zero=True))(pos_in)\n",
        "\n",
        "# concatenating them word+char+pos\n",
        "x = concatenate([emb_word, char_enc, pos_word])\n",
        "\n",
        "#passing to Bi-LSTM layer\n",
        "o=(Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True, dropout=0.2)))(x)\n",
        "#print(o)\n",
        "\n",
        "#Self attention layer\n",
        "o = (SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                       kernel_regularizer=keras.regularizers.l2(1e-4),\n",
        "                       bias_regularizer=keras.regularizers.l1(1e-4),\n",
        "                       attention_regularizer_weight=1e-4,\n",
        "                       name='Attention'))(o)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HE6EmtWXiRU"
      },
      "source": [
        "#CRF layer\n",
        "crf = CRF(len(class_labels), sparse_target=True,name='crf')\n",
        "o=(crf)(o)\n",
        "model=Model([word_in,char_in,pos_in],o)\n",
        "model.compile('adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJA-sR6HXkwr",
        "outputId": "90601bed-360b-4fb2-afba-5e51e29cfdab"
      },
      "source": [
        "EPOCHS = 5\n",
        "model.fit([X_train,np.array(X_char_train).reshape(len(X_char_train),max_len,max_len_char),x_pos_train.reshape(len(x_pos_train),max_len)],np.array(y_train).reshape(len(y_train), max_len, 1),batch_size=32, epochs=EPOCHS, validation_split=0.1, verbose=1)\n",
        "model.save('/content/drive/My Drive/explo/bhoj_explo_chunk.bin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3985 samples, validate on 443 samples\n",
            "Epoch 1/5\n",
            "3985/3985 [==============================] - 1382s 347ms/step - loss: 172.7714 - crf_viterbi_accuracy: 0.3522 - val_loss: 145.2792 - val_crf_viterbi_accuracy: 0.4196\n",
            "Epoch 2/5\n",
            "3985/3985 [==============================] - 1355s 340ms/step - loss: 171.8562 - crf_viterbi_accuracy: 0.6717 - val_loss: 144.2400 - val_crf_viterbi_accuracy: 0.8264\n",
            "Epoch 3/5\n",
            "3985/3985 [==============================] - 1358s 341ms/step - loss: 171.2712 - crf_viterbi_accuracy: 0.8968 - val_loss: 143.9212 - val_crf_viterbi_accuracy: 0.9233\n",
            "Epoch 4/5\n",
            "3985/3985 [==============================] - 1375s 345ms/step - loss: 171.1501 - crf_viterbi_accuracy: 0.9216 - val_loss: 143.8838 - val_crf_viterbi_accuracy: 0.9223\n",
            "Epoch 5/5\n",
            "3985/3985 [==============================] - 1381s 346ms/step - loss: 171.0862 - crf_viterbi_accuracy: 0.9309 - val_loss: 143.8598 - val_crf_viterbi_accuracy: 0.9217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPo0pyGnXs3g",
        "outputId": "34068161-0d68-4e9e-dc5e-22f527101879"
      },
      "source": [
        "print(len(X_char_train))\n",
        "arr = np.array(X_char_train)\n",
        "print(arr.shape)\n",
        "\n",
        "print(len(X_char_test))\n",
        "arr_test = np.array(X_char_test)\n",
        "print(arr_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4428\n",
            "(4428, 369, 23)\n",
            "1899\n",
            "(1899, 369, 23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o19zJDVEXzCl",
        "outputId": "b35ee102-e698-4804-fece-4b9481827625"
      },
      "source": [
        "#validation\n",
        "model_json = model.to_json()\n",
        "with open(\"model_mai_only.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/drive/My Drive/explo/model_only_bhoj.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "l4=[0]*max_len_char\n",
        "l4=numpy.asarray(l4)\n",
        "y= model.predict([X_test,np.array(X_char_test).reshape((len(X_char_test),max_len, max_len_char)),x_pos_test.reshape(len(x_pos_test),max_len)])\n",
        "y= y.argmax(-1)\n",
        "y_pred=[]\n",
        "test_y_true=[]\n",
        "ctest_y_pred=[]\n",
        "ctest_y_true=[]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD7nnJyJX4bq",
        "outputId": "df3919db-fb4a-4f77-e16b-b2e75b390eb5"
      },
      "source": [
        "#removing padding from the validation\n",
        "for i in range(len(X_test)):\n",
        "    l=[]\n",
        "    l1=[]\n",
        "    for j in range(len(X_test[i])):\n",
        "        if (X_test[i][j]==0):\n",
        "            continue\n",
        "        l.append(y[i][j])\n",
        "        l1.append(y_test[i][j][0])\n",
        "        ctest_y_pred.append(y[i][j])\n",
        "        ctest_y_true.append(y_test[i][j][0])\n",
        "    y_pred.append(l)\n",
        "    test_y_true.append(l1)\n",
        "\n",
        "#converting to numpy array\n",
        "test_y_pred=numpy.asarray(y_pred)\n",
        "test_y_true=numpy.asarray(test_y_true)\n",
        "\n",
        "#writing output for validation\n",
        "f1=open(output_file,'w')\n",
        "for i in range(len(test_y_pred)):\n",
        "    for j in range(len(test_y_pred[i])):\n",
        "        s='|'+'\\t'+'NN'+'\\t'+str(class_labels[test_y_pred[i][j]])+'\\t'+str(class_labels[test_y_true[i][j]])\n",
        "        f1.write(s+'\\n')\n",
        "    f1.write('\\n')\n",
        "f1.flush()\n",
        "f1.close()\n",
        "print('\\n---- Result of BiLSTM-CRF ----\\n')\n",
        "classification_report(ctest_y_true, ctest_y_pred, class_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- Result of BiLSTM-CRF ----\n",
            "\n",
            "                   recall precision  f1-score   support\n",
            "\n",
            "B-BLK                0.74      0.68      0.71       355\n",
            "B-CCP                0.99      0.95      0.97       747\n",
            "B-JJP                0.00      0.00      0.00       160\n",
            "B-NEGP               0.01      1.00      0.03        73\n",
            "B-NNP                0.00      0.00      0.00         0\n",
            "B-NP                 0.97      0.92      0.94      6846\n",
            "B-NST                0.00      0.00      0.00         0\n",
            "B-PRP                0.00      0.00      0.00         0\n",
            "B-RBP                0.09      0.58      0.15        81\n",
            "B-RP                 0.00      0.00      0.00         0\n",
            "B-VGF                0.96      0.89      0.92      3205\n",
            "B-VGINF              0.00      0.00      0.00         9\n",
            "B-VGNF               0.29      0.53      0.38       394\n",
            "B-VGNN               0.00      0.00      0.00         4\n",
            "I-BLK                0.27      0.57      0.37        63\n",
            "I-CCP                0.00      0.00      0.00        11\n",
            "I-JJP                0.00      0.00      0.00       152\n",
            "I-NEGP               0.00      0.00      0.00        27\n",
            "I-NP                 0.96      0.96      0.96      9055\n",
            "I-NST                0.00      0.00      0.00         0\n",
            "I-PRP                0.00      0.00      0.00         0\n",
            "I-RBP                0.03      1.00      0.05        40\n",
            "I-RP                 0.00      0.00      0.00         0\n",
            "I-VGF                0.95      0.90      0.93      2940\n",
            "I-VGINF              0.00      0.00      0.00        10\n",
            "I-VGNF               0.48      0.52      0.50       330\n",
            "I-VGNN               0.00      0.00      0.00         1\n",
            "O                    0.99      0.99      0.99      1460\n",
            "\n",
            "avg / total          0.92      0.90      0.91     25963\n",
            "\n",
            "Accuracy: 0.9210414821091554\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l4ccyBCJ6oW"
      },
      "source": [
        "## **vec mapping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meN7zWaHLW3Z",
        "outputId": "4b16ff46-27c3-4cc3-e9ea-80f809ebafd6"
      },
      "source": [
        "# below used file is the file same as used for training of data ( word - NER tag ) type\n",
        "f = open ('/content/drive/My Drive/explo/bhoj_chunk.txt','r')\n",
        "lines = f.readlines()\n",
        "print(lines[0].split('\\t'))\n",
        "\n",
        "words = []\n",
        "sents = []\n",
        "sent = []\n",
        "for line in lines:\n",
        "  line = line.split('\\t')\n",
        "  if line[0]!='\\n':\n",
        "    word = line[0].split('\\ufeff')\n",
        "    if len(word)>1:\n",
        "      word = word[1]\n",
        "    else:\n",
        "      word = word[0]\n",
        "    if word == '।':\n",
        "      sent.append(word)\n",
        "      sents.append(sent)\n",
        "      sent=[]\n",
        "    else:\n",
        "      sent.append(word)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "word2vec = Word2Vec(sents, size=100,window=5,min_count=2, negative=10,sg=1)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeffसुकन्या', 'NNP', 'B-NP\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COKJ500bz5CM",
        "outputId": "630e80d9-b586-4756-f08d-d5fca890f2df"
      },
      "source": [
        "print(len(word2vec.wv.vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iokpbk0x9r7"
      },
      "source": [
        "f = open ('/content/drive/My Drive/explo/bhoj.txt','r')\n",
        "embeddings_index={}\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "embedding_matrix = np.zeros((len(vocab), 100))\n",
        "i = 0\n",
        "for word in vocab:\n",
        "    # values = line.split()\n",
        "    # word = values[0]\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMIWqcYTt0AU",
        "outputId": "6bcfecb4-7b46-4b6e-b799-2d63eefbe785"
      },
      "source": [
        "# model = Sequential()\n",
        "# pretrained_weights = word2vec.wv.syn0\n",
        "# model.add(Embedding(input_dim=len(word2vec.wv.vocab)+1, output_dim=100,weights =[embedding_matrix] ,mask_zero=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y2eu7E_sHUi",
        "outputId": "8512692b-1575-4531-d44f-2435fd47f1d7"
      },
      "source": [
        "from keras import backend as K\n",
        "# word embedding\n",
        "\n",
        "word_in=Input(shape=(X_train.shape[1],),name='word_in')\n",
        "emb_word=(Embedding(len(vocab), EMBED_DIM//4, weights = [embedding_matrix], mask_zero=True))(word_in)\n",
        "\n",
        "print(X_train.shape[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4GTup_YJ59L"
      },
      "source": [
        "# charcter embedding\n",
        "char_in = Input(shape=(X_char_train.shape[1],X_char_train.shape[2]), name='char_in')\n",
        "emb_char=(TimeDistributed(Embedding(input_dim=len(characters),output_dim=10,input_length=max_len_char,mask_zero=True)))(char_in)\n",
        "char_enc=(TimeDistributed(LSTM(units=EMBED_DIM//4,return_sequences=False)))(emb_char)\n",
        "\n",
        "# pos_tag embedding \n",
        "pos_in=Input(shape=(x_pos_train.shape[1],),name='pos_in')\n",
        "pos_word=(Embedding(len(pos_tags), EMBED_DIM//2, mask_zero=True))(pos_in)\n",
        "\n",
        "# concatenating them word+char+pos\n",
        "x = concatenate([emb_word, char_enc, pos_word])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y_P4lt9LlkJ"
      },
      "source": [
        "#passing to Bi-LSTM layer\n",
        "o=(Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True, dropout=0.2)))(x)\n",
        "#print(o)\n",
        "\n",
        "#Self attention layer\n",
        "o = (SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                       kernel_regularizer=keras.regularizers.l2(1e-4),\n",
        "                       bias_regularizer=keras.regularizers.l1(1e-4),\n",
        "                       attention_regularizer_weight=1e-4,\n",
        "                       name='Attention'))(o)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVuqvZhf40Bo"
      },
      "source": [
        "#CRF layer\n",
        "crf = CRF(len(class_labels), sparse_target=True,name='crf')\n",
        "o=(crf)(o)\n",
        "model=Model([word_in,char_in,pos_in],o)\n",
        "model.compile('adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIi1Zyue472Q",
        "outputId": "a6aee514-5344-4235-bf4f-7f6c7cc45ceb"
      },
      "source": [
        "EPOCHS = 5\n",
        "model.fit([X_train,np.array(X_char_train).reshape(len(X_char_train),max_len,max_len_char),x_pos_train.reshape(len(x_pos_train),max_len)],np.array(y_train).reshape(len(y_train), max_len, 1),batch_size=32, epochs=EPOCHS, validation_split=0.1, verbose=1)\n",
        "model.save('/content/drive/My Drive/explo/bhoj_explo_chunk.bin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3985 samples, validate on 443 samples\n",
            "Epoch 1/5\n",
            "3985/3985 [==============================] - 1390s 349ms/step - loss: 172.3186 - crf_viterbi_accuracy: 0.5177 - val_loss: 144.4021 - val_crf_viterbi_accuracy: 0.7409\n",
            "Epoch 2/5\n",
            "3985/3985 [==============================] - 1365s 342ms/step - loss: 171.4978 - crf_viterbi_accuracy: 0.8115 - val_loss: 144.0795 - val_crf_viterbi_accuracy: 0.8923\n",
            "Epoch 3/5\n",
            "3985/3985 [==============================] - 1338s 336ms/step - loss: 171.2128 - crf_viterbi_accuracy: 0.9069 - val_loss: 143.9287 - val_crf_viterbi_accuracy: 0.9218\n",
            "Epoch 4/5\n",
            "3985/3985 [==============================] - 1336s 335ms/step - loss: 171.1123 - crf_viterbi_accuracy: 0.9245 - val_loss: 143.8649 - val_crf_viterbi_accuracy: 0.9241\n",
            "Epoch 5/5\n",
            "3985/3985 [==============================] - 1368s 343ms/step - loss: 171.0722 - crf_viterbi_accuracy: 0.9363 - val_loss: 143.8416 - val_crf_viterbi_accuracy: 0.9283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyLk1CdMRQ6d",
        "outputId": "adb29fec-f8f8-4cd8-a535-8cfb56d0679a"
      },
      "source": [
        "#validation\n",
        "model_json = model.to_json()\n",
        "with open(\"model_bhoj.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "# model1_new.save_weights(\"/content/drive/My Drive/explo/model_bhoj_fine.h5\")\n",
        "# print(\"Saved model to disk\")\n",
        "l4=[0]*max_len_char\n",
        "l4=numpy.asarray(l4)\n",
        "y= model.predict([X_test,np.array(X_char_test).reshape((len(X_char_test),max_len, max_len_char)),x_pos_test.reshape(len(x_pos_test),max_len)])\n",
        "y= y.argmax(-1)\n",
        "y_pred=[]\n",
        "test_y_true=[]\n",
        "ctest_y_pred=[]\n",
        "ctest_y_true=[]\n",
        "\n",
        "#removing padding from the validation\n",
        "for i in range(len(X_test)):\n",
        "    l=[]\n",
        "    l1=[]\n",
        "    for j in range(len(X_test[i])):\n",
        "        if (X_test[i][j]==0):\n",
        "            continue\n",
        "        l.append(y[i][j])\n",
        "        l1.append(y_test[i][j][0])\n",
        "        ctest_y_pred.append(y[i][j])\n",
        "        ctest_y_true.append(y_test[i][j][0])\n",
        "    y_pred.append(l)\n",
        "    test_y_true.append(l1)\n",
        "\n",
        "#converting to numpy array\n",
        "test_y_pred=numpy.asarray(y_pred)\n",
        "test_y_true=numpy.asarray(test_y_true)\n",
        "\n",
        "#writing output for validation\n",
        "f1=open(output_file,'w')\n",
        "for i in range(len(test_y_pred)):\n",
        "    for j in range(len(test_y_pred[i])):\n",
        "        s='|'+'\\t'+'NN'+'\\t'+str(class_labels[test_y_pred[i][j]])+'\\t'+str(class_labels[test_y_true[i][j]])\n",
        "        f1.write(s+'\\n')\n",
        "    f1.write('\\n')\n",
        "f1.flush()\n",
        "f1.close()\n",
        "print('\\n---- Result of BiLSTM-CRF ----\\n')\n",
        "classification_report(ctest_y_true, ctest_y_pred, class_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---- Result of BiLSTM-CRF ----\n",
            "\n",
            "                   recall precision  f1-score   support\n",
            "\n",
            "B-BLK                0.67      0.85      0.75       355\n",
            "B-CCP                0.99      0.99      0.99       747\n",
            "B-JJP                0.02      0.60      0.04       160\n",
            "B-NEGP               0.00      0.00      0.00        73\n",
            "B-NNP                0.00      0.00      0.00         0\n",
            "B-NP                 0.97      0.93      0.95      6846\n",
            "B-NST                0.00      0.00      0.00         0\n",
            "B-PRP                0.00      0.00      0.00         0\n",
            "B-RBP                0.86      0.78      0.82        81\n",
            "B-RP                 0.00      0.00      0.00         0\n",
            "B-VGF                0.98      0.86      0.92      3205\n",
            "B-VGINF              0.00      0.00      0.00         9\n",
            "B-VGNF               0.00      0.00      0.00       394\n",
            "B-VGNN               0.00      0.00      0.00         4\n",
            "I-BLK                0.44      0.65      0.53        63\n",
            "I-CCP                0.00      0.00      0.00        11\n",
            "I-JJP                0.04      1.00      0.08       152\n",
            "I-NEGP               0.00      0.00      0.00        27\n",
            "I-NP                 0.97      0.96      0.97      9055\n",
            "I-NST                0.00      0.00      0.00         0\n",
            "I-PRP                0.00      0.00      0.00         0\n",
            "I-RBP                0.47      0.90      0.62        40\n",
            "I-RP                 0.00      0.00      0.00         0\n",
            "I-VGF                0.99      0.87      0.92      2940\n",
            "I-VGINF              0.00      0.00      0.00        10\n",
            "I-VGNF               0.00      0.20      0.01       330\n",
            "I-VGNN               0.00      0.00      0.00         1\n",
            "O                    1.00      1.00      1.00      1460\n",
            "\n",
            "avg / total          0.93      0.90      0.90     25963\n",
            "\n",
            "Accuracy: 0.925663444132034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}